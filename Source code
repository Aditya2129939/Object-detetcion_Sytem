import os
import collections
import pandas as pd
import numpy as np
import functools
import matplotlib.pyplot as plt
import cv2
from sklearn import preprocessing
import xml.etree.ElementTree as ET
import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2
import torch
import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator
from torch.utils.data import DataLoader, Dataset
from torch.utils.data import SequentialSampler
BASE_PATH = "../input/pascal-voc-2012/VOC2012"
XML_PATH = os.path.join(BASE_PATH, "Annotations")
IMG_PATH = os.path.join(BASE_PATH, "JPEGImages")
XML_FILES = [os.path.join(XML_PATH, f) for f in os.listdir(XML_PATH)]

#Extract info. from xml files:-
class XmlParser(object):
 def __init__(self,xml_file):
 self.xml_file = xml_file
 self._root = ET.parse(self.xml_file).getroot()
 self._objects = self._root.findall("object")
 # path to the image file as describe in the xml file
 self.img_path = os.path.join(IMG_PATH, self._root.find('filename').text)
 # image id
 self.image_id = self._root.find("filename").text
 # names of the classes contained in the xml file
 self.names = self._get_names()
 # coordinates of the bounding boxes
 self.boxes = self._get_bndbox()
 def parse_xml(self):
 """"Parse the xml file returning the root."""
 
 tree = ET.parse(self.xml_file)
 return tree.getroot()
 def _get_names(self):
 names = []
 for obj in self._objects:
 name = obj.find("name")
 names.append(name.text)
 return np.array(names)
 def _get_bndbox(self):
 boxes = []
 for obj in self._objects:
 coordinates = []
 bndbox = obj.find("bndbox")
 coordinates.append(np.int32(bndbox.find("xmin").text))
 coordinates.append(np.int32(np.float32(bndbox.find("ymin").text)))
 coordinates.append(np.int32(bndbox.find("xmax").text))
 coordinates.append(np.int32(bndbox.find("ymax").text))
 boxes.append(coordinates)
 return np.array(boxes)
Make dataframe from extracted information:- 
def xml_files_to_df(xml_files):

 """"Return pandas dataframe from list of XML files."""
 
 names = []
 boxes = []
 image_id = []
 xml_path = []
 img_path = []
 for file in xml_files:
 xml = XmlParser(file)
 names.extend(xml.names)
 boxes.extend(xml.boxes)
 image_id.extend([xml.image_id] * len(xml.names))
 xml_path.extend([xml.xml_file] * len(xml.names))
 img_path.extend([xml.img_path] * len(xml.names))
 a = {"image_id": image_id,
 "names": names,
 "boxes": boxes,
 "xml_path":xml_path,
 "img_path":img_path}
 
 df = pd.DataFrame.from_dict(a, orient='index')
 df = df.transpose()
 
 return df
df = xml_files_to_df(XML_FILES)
df.head()
# classes need to be in int form so we use LabelEncoderforthis task
enc = preprocessing.LabelEncoder()
df['labels'] = enc.fit_transform(df['names'])
df['labels'] = np.stack(df['labels'][i]+1 for i in range(len(df['labels'])))
# make dictionary for class objects so we can call objects by their keys.
classes=
{1:'aeroplane',2:'bicycle',3:'bird',4:'boat',5:'bottle',6:'bus',7:'car',8:'cat',9:'chair',10:'cow',11:'dini
ngtable',12:'dog',13:'horse',14:'motorbike',15:'person',16:'pottedplant',17:'sheep',18:'sofa',19:'tra
in',20:'tvmonitor'}
In [10]:
# bounding box coordinates point need to be in separate columns
df['xmin'] = -1
df['ymin'] = -1
df['xmax'] = -1
df['ymax'] = -1
df[['xmin','ymin','xmax','ymax']]=np.stack(df['boxes'][i] for i in range(len(df['boxes'])))
df.drop(columns=['boxes'], inplace=True)
df['xmin'] = df['xmin'].astype(np.float)
df['ymin'] = df['ymin'].astype(np.float)
df['xmax'] = df['xmax'].astype(np.float)

df['ymax'] = df['ymax'].astype(np.float)
In [11]:
linkcode
# drop names column since we dont need it anymore
df.drop(columns=['names'], inplace=True)
df.head()
Separate train and validation data:- 
image_ids = df['img_id'].unique()
valid_ids = image_ids[-4000:]
train_ids = image_ids[:-4000]
len(train_ids)
linkcode
valid_df = df[df['img_id'].isin(valid_ids)]
train_df = df[df['img_id'].isin(train_ids)]
valid_df.shape, train_df.shape
Make dataset by Dataset Module:- 
 
 def __init__(self, dataframe, image_dir, transforms=None):
 super().__init__()
 
 self.image_ids = dataframe['img_id'].unique()
 self.df = dataframe
 self.image_dir = image_dir
 self.transforms = transforms
 
 def __getitem__(self, index: int):
 image_id = self.image_ids[index]
 records = self.df[self.df['img_id'] == image_id]
 
 image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)
 image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)
 image /= 255.0
 rows, cols = image.shape[:2]
 
 boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values
 
 
 area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
 area = torch.as_tensor(area, dtype=torch.float32)
 
 label = records['labels'].values
 labels = torch.as_tensor(label, dtype=torch.int64)

 # suppose all instances are not crowd
 iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)
 
 target = {}
 target['boxes'] = boxes
 target['labels'] = labels
 # target['masks'] = None
 target['image_id'] = torch.tensor([index])
 target['area'] = area
 target['iscrowd'] = iscrowd
 
 if self.transforms:
 sample = {
 'image': image,
 'bboxes': target['boxes'],
 'labels': labels
 }
 sample = self.transforms(**sample)
 image = sample['image']
 
 target['boxes'] = torch.stack(tuple(map(torch.tensor,
zip(*sample['bboxes'])))).permute(1,0)
 
 return image, target
 
 def __len__(self) -> int:
 return self.image_ids.shape[0]
In [16]:
def get_transform_train():
 return A.Compose([
 A.HorizontalFlip(p=0.5),
 A.RandomBrightnessContrast(p=0.2),
 ToTensorV2(p=1.0)
 ], bbox_params={'format':'pascal_voc', 'label_fields': ['labels']})
def get_transform_valid():
 return A.Compose([
 ToTensorV2(p=1.0)
 ], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})
In [17]:
linkcode
def collate_fn(batch):
 return tuple(zip(*batch))
train_dataset = VOCDataset(train_df, IMG_PATH , get_transform_train())
valid_dataset = VOCDataset(valid_df, IMG_PATH, get_transform_valid())

# split the dataset in train and test set
indices = torch.randperm(len(train_dataset)).tolist()
train_data_loader = DataLoader(
 train_dataset,
 batch_size=4,
 shuffle=True,
 num_workers=4,
 collate_fn=collate_fn
)
valid_data_loader = DataLoader(
 valid_dataset,
 batch_size=4,
 shuffle=False,
 num_workers=4,
 collate_fn=collate_fn
)
In [18]:
linkcode
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
View sample:-
images, targets= next(iter(train_data_loader))
images = list(image.to(device) for image in images)
targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
plt.figure(figsize=(20,20))
for i, (image, target) in enumerate(zip(images, targets)):
 plt.subplot(2,2, i+1)
 boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)
 sample = images[i].permute(1,2,0).cpu().numpy()
 names = targets[i]['labels'].cpu().numpy().astype(np.int64)
 for i,box in enumerate(boxes):
 cv2.rectangle(sample,
 (box[0], box[1]),
 (box[2], box[3]),
 (0, 0, 220), 2)
 cv2.putText(sample, classes[names[i]], (box[0],box[1]+15),cv2.FONT_HERSHEY_COMPLEX
,0.5,(0,220,0),1,cv2.LINE_AA) 
 plt.axis('off')
 plt.imshow(sample)
Download modules for model training:-
# !pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'
# !git clone https://github.com/pytorch/vision.git
# !cd vision;cp references/detection/utils.py ../;cp references/detection/transforms.py ../;cp
references/detection/coco_eval.py ../;cp references/detection/engine.py ../;cp
references/detection/coco_utils.py ../

linkcode
from engine import train_one_epoch, evaluate
import utils
Train object detection model:-
num_epochs = 2
for epoch in range(num_epochs):
 # train for one epoch, printing every 10 iterations
 train_one_epoch(model, optimizer, train_data_loader, device, epoch, print_freq=10)
 # update the learning rate
 lr_scheduler.step()
 # evaluate on the test dataset
 evaluate(model, valid_data_loader, device=device)
linkcode
torch.save(model.state_dict(), 'faster_rcnn_state.pth')
Test model:-
# load a model; pre-trained on COCO
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False,
pretrained_backbone=False)
WEIGHTS_FILE = "./faster_rcnn_state.pth"
num_classes = 21
# get number of input features for the classifier
in_features = model.roi_heads.box_predictor.cls_score.in_features
# replace the pre-trained head with a new one
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
# Load the traines weights
model.load_state_dict(torch.load(WEIGHTS_FILE))
model = model.to(device)
In [40]:
def obj_detector(img):
 img = cv2.imread(img, cv2.IMREAD_COLOR)
 img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)
 img /= 255.0
 img = torch.from_numpy(img)
 img = img.unsqueeze(0)
 img = img.permute(0,3,1,2)
 
 model.eval()

 detection_threshold = 0.70
 
 img = list(im.to(device) for im in img)
 output = model(img)
 for i , im in enumerate(img):
 boxes = output[i]['boxes'].data.cpu().numpy()
 scores = output[i]['scores'].data.cpu().numpy()
 labels = output[i]['labels'].data.cpu().numpy()
 labels = labels[scores >= detection_threshold]
 boxes = boxes[scores >= detection_threshold].astype(np.int32)
 scores = scores[scores >= detection_threshold]
 boxes[:, 2] = boxes[:, 2] - boxes[:, 0]
 boxes[:, 3] = boxes[:, 3] - boxes[:, 1]
 
 sample = img[0].permute(1,2,0).cpu().numpy()
 sample = np.array(sample)
 boxes = output[0]['boxes'].data.cpu().numpy()
 name = output[0]['labels'].data.cpu().numpy()
 scores = output[0]['scores'].data.cpu().numpy()
 boxes = boxes[scores >= detection_threshold].astype(np.int32)
 names = name.tolist()
 
 return names, boxes, sample
In [41]:
linkcode
pred_path = "../input/data-images"
pred_files = [os.path.join(pred_path,f) for f in os.listdir(pred_path)]
plt.figure(figsize=(20,60))
for i, images in enumerate(pred_files):
 if i > 19:break
 plt.subplot(10,2,i+1)
 names,boxes,sample = obj_detector(images)
 for i,box in enumerate(boxes):
 cv2.rectangle(sample,
 (box[0], box[1]),
 (box[2], box[3]),
 (0, 220, 0), 2)
 cv2.putText(sample, classes[names[i]], (box[0],box[1]-
5),cv2.FONT_HERSHEY_COMPLEX ,0.7,(220,0,0),1,cv2.LINE_AA) 
 plt.axis('off')
 plt.imshow(sample)
# plt.savefig('save_image.png', bbox_inches='tight') # if you want to save resul
